# Core Trajectory Invariant

> **Purpose**: This document defines the non‑negotiable directions of my long‑term growth.
> It exists to prevent drift, doubt, and identity collapse under external pressure (jobs, companies, markets, or crises).

This is not a goal list. It is a **constitutional guideline**.

## The Prime Invariant

> **No matter what happens externally, I advance my understanding and capability in neural networks and systems every week.**

This invariant holds regardless of:

* employment status
* job search pressure
* location or environment

External circumstances may change *pace*, but never *direction*.

## The Three Core Directions

These three directions define the full surface of my trajectory.  
They are **not equal in role**, and they must **not be pursued simultaneously with equal intensity**.

## 1. Autograd & Linear Algebra

**Role**: Root truth / identity layer  
**Question it answers**: *Do I truly understand how intelligence is built?*

### Scope

* Automatic differentiation
* Backpropagation mechanics
* Computational graphs
* Gradients as linear maps
* Vector–Jacobian and Jacobian–vector products
* Linear algebra foundations of neural networks

### Form

* From‑scratch implementations
* Minimal, explicit code
* Mathematical derivations tied directly to code
* Notes written in my own words

### Cadence

* **Weekly — non‑negotiable**
* Even 30–60 minutes counts

### Rules

* Depth over breadth
* No performance optimization unless conceptually required
* Confusion is a success state, not a failure

> This is the **North Star**.
> Everything else is downstream of this layer.

## 2. Machine Learning Systems Practice

**Role**: Translation layer (intelligence → reality)  
**Question it answers**: *Can this intelligence survive contact with the real world?*

### Scope

* Training a model end‑to‑end
* Data pipelines
* Evaluation and metrics
* Serving and inference
* Monitoring, drift, and updates
* System boundaries and failure modes

### Form

* Small, complete systems
* Clear contracts and interfaces
* Reproducible training and inference
* Explicit assumptions and constraints

### Cadence

* **Project‑based / episodic**
* Activated when capacity allows

### Rules

* One system at a time
* Finish → document → close
* No premature scaling

> This is where understanding becomes *useful*.

## 3. Linux & General Systems Mastery

**Role**: Substrate / execution layer  
**Question it answers**: *Can I build and operate real systems without friction?*

### Scope

* Linux fundamentals
* Filesystems, processes, permissions
* Shell tooling (grep, awk, sed, tmux, etc.)
* Logging, debugging, and observability

### Form

* Daily or weekly exposure
* Habit formation
* Muscle memory over theory

### Cadence

* **Low but continuous**

### Rules

* Should feel boring over time
* No heroics
* Automation over repetition

> This layer removes friction.
> It should never dominate attention.

## How These Directions Interact

```
Autograd & Linear Algebra
        ↓
ML Systems Practice
        ↓
Linux / System Substrate
```

* Autograd defines **what is possible**
* ML systems define **what is viable**
* Linux defines **what is executable**

Misalignment happens when:

* systems outrun understanding
* execution outruns design

This document exists to prevent that.

## Weekend Rule

Weekends are for **identity reinforcement**, not urgency.

Allowed:

* Autograd exploration
* Light linear algebra study
* Gentle Linux practice

Not allowed:

* Deadline pressure
* Career comparison
* Outcome‑based self‑evaluation

## Anti‑Drift Check

When doubt appears, ask:

1. Did I touch neural network understanding this week?
2. Did I move one layer deeper, even slightly?

If yes → trajectory is intact.

## Final Reminder

> **I am not optimizing for short‑term success.**
> **I am building irreversible understanding.**

This trajectory compounds quietly.

Return to this document whenever uncertainty tries to redefine who I am.
