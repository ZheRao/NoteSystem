

wx

    - a matrix is a recipe for generating new features from your input 

    - mental image - picture a 3D project 
        -- rotate -> same coordinates, different basis (invertible)
        -- flatten into 2D -> projection 
        -- extrude into 4D -> embedding 
        -- stretch and distort -> general linear transformation 
        -- learn new axes to represent it -> neural network layer 

    - wi^T x = how much x aligns with direction wi

        -- components
            --- x: raw input (image patch, sound, token embedding, etc.)
            --- wi: a learned direction (edge detector, pattern detector, etc.)
            --- wi^T x: the response of input x to that detector 
        
        -- when dot product is high 
            --- x has a strong horizontal edge 
            --- x contains the shape 
            --- x matches the pattern 
            --- x aligns with the learned concept 
        
        -- if it is low 
            --- no match 
            --- no alignment 
            --- not that feature 
        
        -- dot product is "alignment"

    - transformation from 3D to 5D

        -- "let me feed this 3D thing into a machine that spits out a 5D description of it"

        -- the result tensor will not span 5D space - but it's not the goal - the goal is to create a space where useful features live linearly 
    
    - non-linear activations 

        -- concept 
            --- linear layers create new coordinates where patterns look cleaner 
            --- non-linear layers let the network bend space to fit the data 
        
        -- ReLU kills 3/4 of space 
            --- linear transformations push irrelevant information into negative quadrants and preserve only relevant information in the positive quadrant, so only relevant info pass down 
            --- even if ReLU kills some useful data, it can recover - because network has many neurons in each layer 
            


geometric meaning of neural network calculations

    - basic interpretation

        -- each row vector of the weight matrix A represent a direction that will be compared against observation b (through dot product)
            
            --- think of b as a word - each row of A is asking a question "is this word about time? or is this word about a person?"
                
                ---- or For sentiment:

                        -> One row might detect ‚Äúnegation‚Äù (not, never, no)
                        -> One row might detect ‚Äúanger tone‚Äù
                        -> One row might detect ‚Äúpraise‚Äù
                        -> One row might detect ‚Äúsarcasm‚Äù
                        -> One row might detect ‚Äúsubjectivity‚Äù
                        -> One row might detect ‚Äúpolarity shift‚Äù

            --- a neuron = a feature detector 

                ---- think of each row as a pattern it wants to see in input 

                ---- it fires strongly if 
                        -> the input contains that pattern 
                        -> the input points in that direction 
                
                ---- it stays silent if 
                        -> the input is irrelevant 
                        -> the input points orthogonal to that direction 
            
            --- first layer h = Ax expresses x in the new basis defined by A's rows 
                ---- h is the space that encode the description of the word based on A 
                ---- first value says time, second value says people 
            
            --- second layer h2 = Ah extract higher order meaning 
                ---- they are new directions, but now over h
                ---- now features can become: love, regret, excitment, danger, truthfulness, affection, betrayal, sentiment, sarcasm, ...
            
            --- after training, the final space is intelligence - the last few layers hold a space where 

                -> similar meanings come close
                -> contradictions push apart
                -> analogies become linear paths
                -> syntactic relations become hyperplanes
                -> reasoning steps become geometric translations
                -> concepts become manifolds
                -> categories become clusters
                -> abstractions become subspaces
                -> queries become directions in thought space

        -- after training, SGD rotates row vectors in high-dimensional space until they point at "useful" directions for classification or representation 

            --- during training, SGD adjusts every row vector in the weight matrix until it 

                ---- detects some feature 
                ---- aligns strongly with inputs containing that feature 
                ---- aligns wekaly or negatively with irrelevant inputs 
            
            --- each row becomes a basis vector of the learned representation space 
                ---- that asks a specific question at different levels for different abstractions

            --- during training, SGD literally rotates row vectors until they point toward useful concepts 

            --- during training the network extracts knowledge - during inference it applies it 
                ---- training = extracting geometric structure 
                ---- inference = applying that structure to new inputs 
            
            --- training builds 
                ---- directions (features)
                ---- subspaces (concepts)
                ---- distances (similarities)
                ---- manifolds (clusters)
                ---- transformations (relations)

        -- just like CNN

            --- a CNN filter is also just a vector, and it computes a dot product with a patch of the image to determine alignment of the feature inside the image patch 

            --- filters ask "does this patch look like my pattern?"
        
        -- just like attention 

            --- same geometry, same inner product 
                ---- Q - the direction of attention you are sending out 
                ---- K - the direction other tokens want to be recognized for 
                ---- V - the content carried if the match is good 

            --- gist 
                ---- each token is literally asking "who in this sequence points in the same direction as they query I am sending out?"
    
    - unifying statement

        All neural network layers are just ways of computing learned dot-products between input vectors and learned direction vectors.

            Dense layers ‚Üí rows of A are learned directions
            CNNs ‚Üí filters are learned directions
            Attention ‚Üí Q and K matrices create learned directions
            RNNs ‚Üí hidden-to-hidden weights define learned directions
            Embeddings ‚Üí each embedding vector is a learned direction

        Everything is dot products.
        Everything is similarity.
        Everything is alignment.
        Everything is projections into new spaces.

        Neural networks are basically giant high-dimensional geometry machines.
    
    - unify statement 2 

        A feature is a direction in high-dimensional space.
        Neural networks detect features by measuring how aligned the input is with that direction.

        So if neuron i has row vector ùë§ùëñ, 

            It fires when the input ‚Äúcontains that feature‚Äù
            Because the input lies along that direction
            And dot product measures how much of the input lies in that direction

        Just like CNNs detect edges.
        Just like attention detects relevance.
        Just like PCA finds principal directions.
        Just like embeddings encode semantic relationships.

    - how can each neuron learns independently? I learn dependently!

        Your brain learns dependently because:

            you supervise yourself
            you see the whole system
            you reason about meaning
            you care about coherence

        A neural network doesn‚Äôt.
        It learns mathematically:

            gradient = ‚ÄúI was wrong in this direction‚Äù
            update = ‚Äúrotate slightly toward better direction‚Äù

        No neuron knows ‚Äúmeaning.‚Äù
        No neuron knows the goal.
        No neuron knows the others.

        But collectively?

        They build coherent geometric structures.

        It‚Äôs like 1,000 agents each making tiny corrections‚Ä¶
        and the emergent behavior is intelligence.

        This is why backprop is one of the greatest inventions in history.

































