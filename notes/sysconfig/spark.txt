



orchestrating tasks manually

    - automatic system partition pair with 'mapPartitions'

        -- 'mapPartitions(func)': func runs once per partition, receiving an iterator over all elements in that partition, and must return/yield zero or more results 
            
            --- sidenote: 'foreachPartition(func)' -> only for side effects (writes, logging), func returns nothing
        
            --- preparation: 'parallelize(tasks, numSlices=num_partitions)'
                ---- Spark divides the list into 'num_partitions' slices (roughly evenly)
                ---- each slice becomes one partition -> one task that will call your 'mapPartitions' function once, handing it an iterator over that partition's elements 
                        -> no shuffle happens here; it's a narrow transform
                        -> scheduling: DAGScheduler creates tasks = number of partitions; executors pull tasks and run your function once per partition 
            
            --- pitfall: why 'parallelize([partition1, partition2], numSlices=2)' wouldn't work 
                
                ---- Spark will put one element per partition 
                        -> partition 0 contains [partition1] (a single element, which itself is a list of tasks)
                        -> partition 1 contains [partition2]
                
                ---- so inside 'mapPartitions(func)', the iterator yields one item (a list) for that partition 
                        -> if func is expecting [task1, task2, ...], it got [partition1], which is [[task1, task2, ...]]
                        






























