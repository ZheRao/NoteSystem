


Linux environment

    - Ubuntu - start from scratch

        -- start from /home/user 

        -- 'mkdir projects && cd projects' - now inside /home/user/projects 

        -- 'code .' - launch VS Code inside projects folder 
        
    - VS Code - setup WSL env and install necessary packages including python

        -- one-time WSL setup (global tools only)

            --- 'sudo apt update && sudo apt upgrade -y'
                
                ---- keep Ubuntu tidy by updating package list, then upgrade everything that has updates 

                ---- command breakdown 
                        -> 'sudo': run with administrator privileges (needed to change system packages)
                        -> 'apt update': refreshes the local package index from your configured repositories; it doesn't install anything, just learns what the latest versions are 
                        -> 'apt upgrade -y': downloads and installs newer versions of all currently installed packages that have safe upgrades available; '-y' auto-answers "yes"
                
                ---- benefits
                        -> get the latest security patches and bug fixes promptly 
                        -> installed packages stay consistent with repo versions, reducing dependency conflicts
                        -> clears up "stale index" issues (e.g., 404s when repos change) by refreshing metadata first 
                
                ---- further improvements
                        -> doesn't remove unused packages, to clean leftovers: 'sudo apt autoremove' (and optionally 'sudo apt autoclean')
                        -> 'apt upgrade' won't remove packages; if a dependency change requires removals, use 'sudo apt full-upgrade' (more aggressive)
            
            --- 'sudo apt install -y build-essential curl git pkg-config unzip'

                ---- essentials for building python wheels, talking to git, etc. 

                ---- packages 
                        -> build-essential: a meta-package that pulls in GCC/G++ compilers, 'make', and basic headers/tools needed to compile C/C++ extensions
                        -> curl: command-line HTTP client (download files, hit APIs, fetch install scripts)
                        -> git: version control CLI 
                        -> pkg-config: helper that tells compilers where to find headers/libs for dependencies (e.g., '--cflags', '--libs' for 'libssl', 'libxml2', etc.)
                
                ---- "wheels" in Python 
                        -> a wheel ('.whl') is Python's standard built distribution format (PEP 427)
                        -> think of it as a prebuilt, ready-to-install package; when run 'pip install <package>', pip prefers a wheel because it can just unpack files - no compiling
                        -> if a wheel isn't available for your platform/Python version, pip falls back to a source distribution ('sdist', usually a '.tar.gz')
                            and will try to build a wheel locally; that's when you need compilers and headers - hence 'build-essential' and 'pkg-config'
                
                ---- common add-ons when needed (when something fails)
                        -> Python headers: 'sudo apt install -y python3-dev'
                        -> SSL/cryptography: 'sudo apt install -y libssl-dev libffi-dev'
                        -> compression/CSV/Parquet/etc: 'sudo apt install -y zlib1g-dev libbz2-dev liblzma-dev'
                        -> XML: 'sudo apt install -y libxml2-dev libxslt1-dev'
            
            --- 'sudo pat install -y python3 python3-nenv python3-pip'

                ---- packages 
                        -> 'python3': installs Ubuntu's system Python interpreter (usually /usr/bin/python3)
                        -> 'python3-venv': lets you create virtual environment with 'python3 -m venv'
                        -> 'python3-pip': installs pip, Python's package manager 
                
                ---- purpose 
                        -> Ubuntu already uses Python internally (for system tools, package managers, etc.) and that's called the system python 
                            --> if lives under /usr/bin/python3 and is managed by 'apt', not by 'pip'
                        -> it's fine to use this system Python just to bootstrap - that is, to create isolated environments (venv) and install 'pip'
                        -> you should not install your project libraries (like 'pandas', 'torch', etc.) directly into the system python
                            --> after initialization, all real work happens inside the virtual environments, not the system install 
                
                ---- danger of not isolation
                        -> if you 'pip install' or 'pip uninstall' into /usr/lib/python3/dist-packages, you can break the OS 
                        -> it mixes system and user dependencies - hard to maintain 
                            --> you'll lose control over versions when Ubuntu updates packages 

            --- 'sudo apt install -y pipx' and then 'pipx ensurepath' and then (after reboot)

                ---- sets up pipx, a tool for installing Python CLI apps in their own isolated virtual environments - great for "global" tools like Jupyter - without polluting system Python or Conda envs 

                ---- first command - installs the 'pipx' launcher from Ubuntu repos 

                ---- second command - adds pipx's binary directory (typically ~/.local/bin) to your PATH (by updating your shell rc files)
                        -> after this, newly installed pipx apps are runnable by name 
                
                ---- how pipx works 
                        -> when you run 'pipx install <app>', pipx 
                            --> create a dedicated venv under ~/.local/pipx/venvs/<app>/
                            --> installs the app there 
                            --> symlinks the app's console scripts into ~/.local/bin/
                        -> your Python libraries for projects should still live in your project envs (Conda/venv); use pipx only for standalone command-line tools
                
                ---- 'pip install' vs. 'pipx install'
                        -> 'pip install' installs into the currently active Python environment (your Conda env, venv, or - dangerously - system Python) 
                            --> new package will share that env's packages 
                        -> 'pipx install jupyterlab' creates a dedicated isolated venv just for the new package and exposes 'jupyter-lab' command on your PATH 
                            --> your project packages live elsewhere; Jupyter is just the "app shell"
    
            --- global packages 
                ---- 'pipx install poetry'
                ---- 'pipx install --include-deps jupyter'
                        -> by default, pipx only exposes apps from the top-level packages you install 
                        -> many packages are meta packages (they don't ship a CLI themselves; their dependencies do), 'jupyter' meta package is like that 
                        -> 'jupyter' meta package depends on things like 'jupyter-core', but it doesn't have its own app 
                            --> so 'pipx install jupyter' won't expose the dependency apps unless 'pipx install --include-deps jupyter'

    - Java for PySpark (OpenJDK 17)

        -- 'sudo apt install -y openjdk-17-jdk'
            --- installs the Java 17 JDK (compiler + tools) 
            --- PySpark needs a JVM; Java 11 or 17 is typical 
        
        -- echo 'export JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64' >> ~/.bashrc
            --- appends a line to your '~/.bashrc' setting JAVA_HOME to where OpenJDK 17 lives (on Ubuntu this path is standard)
            --- many tools (including PySpark) look for JAVA_HOME 
            --- 'export' -> marks "JAVA_HOME" variable as an environment variable - visible to child processes (commands or programs launched from this shell)

        -- echo 'export PATH="$JAVA_HOME/bin:$PATH"' >> ~/.bashrc 
            --- appends a line that puts Java's 'bin' directory at the front of your PATH so 'java', 'javac', etc. resolve to this JDK 
        
        -- source ~/.bashrc 
            --- reloads your shell configuration so the new variables take effect immediately 
    
    - install python3.11 with pyenv 

        -- install pyenv 

            --- build deps (Ubuntu/WSL)
                    sudo apt update
                    sudo apt install -y build-essential libssl-dev zlib1g-dev \
                        libbz2-dev libreadline-dev libsqlite3-dev wget curl llvm \
                        libncurses5-dev libncursesw5-dev xz-utils tk-dev \
                        libffi-dev liblzma-dev python3-openssl git
            
            --- install pyenv 'curl https://pyenv.run | bash'

            --- add to ~/.bashrc (idempotent)
                    grep -q 'PYENV_ROOT' ~/.bashrc || cat >> ~/.bashrc <<'EOF'
                    export PYENV_ROOT="$HOME/.pyenv"
                    command -v pyenv >/dev/null || export PATH="$PYENV_ROOT/bin:$PATH"
                    eval "$(pyenv init -)"
                    EOF
                ---- 'grep -q 'PYENV_ROOT' ~/.bashrc' - quietly checks whether .bashrc already contains the text PYENV_ROOT; exit code 0 if found, non-zero if not 
                ---- '||' - "or else" - if 'grep' didn't find it (non-zero), run the right-hand side 
                ---- 'cat >> ~/.bashrc <<'EOF' ... EOF' - appends the following block literally to ~/.bashrc 
                        -> the single-quoted 'EOF' means variables like '$HOME' aren't expanded now - they'll be expanded when '.bashrc' runs 
                        -> 'export PYENV_ROOT="$HOME/.pyenv"' - sets pyenv's install root 
                        -> 'command -v pyenv >/dev/null || export PATH="$PYENV_ROOT/bin:$PATH"' - if 'pyenv' isn't already on PATH (i.e., 'command -v' fails), prepend '$PYENV_ROOT/bin' to PATH
                        -> 'eval "$(pyenv init -)"' - runs pyenv's shell initialization (sets up shims, completions, etc.) in your current shell on future logins 
                ---- '>>' - append output to a file 
                ---- '<<WORD ... WORD' - inline stdin block 
                        -> <<'WORD' (quoted): prevents variable/command expansion inside the block, so '$HOME' stays as literal text until the file is later sourced 
            
            --- reflect changes 
                ---- 'source ~/.bashrc' - activate changes to '.bashrc' or '.bash_profile', etc. without opening a new terminal 
                ---- 'exec $SHELL' - starts a new shell process, destroying the old one 
            
            --- 'pyenv --version' - quick check

            --- install python3.11 (doesn't conflict with other installed python versions)
                ----- 'pyenv install 3.11.9'
                ----- 'pyenv versions'

    - jump start project venv 

        -- mkdir -p project_name && cd project_name 

        -- config venv
            
            --- create venv inside the repo (creates .venv/)
                ---- 'poetry config --local virtualenvs.in-project true'
                        -> '--local' tells Poetry to write the setting into the current project (a 'poetry.toml' file) instead of global config 
                        -> 'poetry.toml' - per-project Poetry settings
                        -> 'pyproject.toml' - your project's pakcage metadata & dependencies

            --- initialize pyproject quickly (edit later)
                    poetry init -n --name "mf_qbo_spark_refactor" \
                        --description "Spark refactor experimentation for QBO ETL" \
                        --license "MIT"

                ---- 'poetry init': creates a 'pyproject.toml' in the current directory to turn it into a Poetry-managed project 
                        -> 'pyproject.toml' - your project's pakcage metadata & dependencies
                ---- '-n' / '--no-interaction': skips the interactive wizard and writes a minimal config using the flags you provide (no prompts)
                ---- '--name': sets your package/project name in metadata 
                ---- '--description': sets the description field
                ---- '--license MIT': sets the license string in metadata (it doesn't create a 'LICENSE' file - do that separately if needed)
            
        -- manage packages 
            
            --- activate specific version of python 
                ---- 'pyenv local 3.11.9' - writes a '.python-version' file in the project root so any shell in this folder uses pyenv's 3.11.9 
                ---- 'python -V' - verify
                ---- 'pyenv which python' - double check 
                ---- 'poetry env use python'
                ---- 'poetry install'

            --- lock onto dependencies already installed 
                ---- 'poetry env use python3.11' - lock to Python 3.11 for PySpark 
            
            --- add packages 
                ---- 'poetry add pyspark==3.5.1 pyarrow pandas orjson rich'
                ---- 'poetry add -D ipykernel'
                        -> 'ipykernel' is the Python kernel for Jupyter - it's the piece that runs Python code when cells are executed 
                            --> Jupyter itself is just the front-end/server (UI, notebook handling, messaging)
                            --> kernels are per-language back-ends that actually execute code 
                        -> keep this separate for each project 
                            --> each project's notebook should run with that project's packages 
        
        -- activate environment 

            --- install kernel: 'poetry add ipykernel'

            --- create kernel: 
                    poetry run python -m ipykernel install --user \
                        --name mf-qbo-spark \
                        --display-name "mf-qbo-spark (poetry)"
                this registers a kernel you can select inside VS Code 
                ---- 'poetry run ...': runs the command inside your project's venv (so the kernel will use that exact interpreter + packages)
                ---- 'python -m ipykernel install': create/updates a kernel spec for Jupyter 
                ---- '--user': installs the kernel spec in your user dir (no sudo), typically under 
                        -> '~/.local/share/jupyter/kernels/<name>/kernel.json'
                ---- '--name mf-qbo-spark': the internal kernel id (folder name), but be unique; reusing will overwrite 
                        -> it becomes the folder name under your Jupyter kernels directory, e.g., '~/.local/share/jupyter/kernels/DLExprEnv/kernel.json'
                ---- '--diplay-name "mf-qbo-spark (poetry)"': the friendly label you'll see in Jupyter / VS Code kernel picker 
            
            --- check kernels - 'jupyter kernelspec list'
            
            --- uninstall old kernels - 'jupyter kernelspec uninstall old_kernel_name'

    - system interaction 
        -- Linux package management (apt)
        -- Python version management (pyend)
        -- virtual-environment management (poetry)
        -- application sandboxing (pipx)
        -- kernel registration & cross-process communication (ipykernel/jupyter)




git

    - create/activate SSH keys for GitHub

            ssh-keygen -t ed25519 -C "your_email@example.com"
            eval "$(ssh-agent -s)"
            ssh-add ~/.ssh/id_ed25519
            cat ~/.ssh/id_ed25519.pub
        
        -- copy the printed key -> GitHub -> Settings -> SSH and GPG keys -> New SSH key

    - one-time global setup  




Spark setup 




Twilio setup 

    - install nvm 

        1) Install nvm

            curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.7/install.sh | bash

            # reload shell (or just close/open terminal)
            source ~/.bashrc  # or ~/.zshrc depending on your shell

        2) Install a stable Node version
            nvm install --lts

        3) Use it
            nvm use --lts

        4) Confirm
            node -v
            npm -v
        
        5) 
            npm install -g twilio-cli

    - login 

        -- 'twilio login'
    
    - comman operations 

        -- create a project 
            --- 'twilio assets:init --service-name sop-project'

        -- activate profile 
            --- 'twilio profiles:list'
            --- 'twilio profiles:use monette-zhe'
        
        -- assets 
            --- 'twilio assets:list'
                ---- 'twilio assets:list -o json > assets/assets_meta.json'
            --- 'twilio assets:upload ./grain_handling.pdf' 

        
        -- upload bulk files 

                cd ~/files
                for f in *.pdf; do
                    twilio assets:upload "$f"
                done
        
        -- install serverless plugin 
            --- 'twilio plugins:install @twilio-labs/plugin-serverless'
        
        -- initialize functions project 

            --- 'twilio serverless:init sop-whatsapp-backend'
            --- 'cd sop-whatsapp-backend'

            --- structure 

                    sop-whatsapp-backend/
                        .env
                        functions/
                            hello-world.js   (sample)
                        assets/
                        package.json

        -- test deploy 
            --- 'cd ~/files/whatsapp-backend'
            --- 'twilio serverless:start'
            --- CTRL+c - cancel
            --- example HTTP request for fake messages 

curl -X POST http://localhost:3000/whatsapp-reply \
    -d "Body=get-chemical, inspection" \
    -d "From=whatsapp:+15551234567" \
    -d "To=whatsapp:+1YOURTWILIONUM" \
    -d "ProfileName=Local Test" \
    -d "NumMedia=0"                
    
        -- actual deploy 
            --- 'cd ~/files/whatsapp-backend'
            --- 'twilio serverless:deploy'
        
        -- logout 
            --- 'twilio logout'




















