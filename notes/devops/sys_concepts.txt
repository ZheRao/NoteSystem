





Python Environments

    - environment vs. virtual environment 

        -- environment 
            --- means a collection of 
                ---- a Python interpreter (the executable 'python')
                ---- a set of installed packages 
                ---- and a specific set of environment variables (like PATH, PYTHONPATH, etc.) that together determine what code runs, and how 
            --- your system Python is itself an environment 
                ---- /usr/bin/python3 
                ---- /usr/lib/python3.10/site-pacakges/
                ---- that's the global environment - all users share it 
        
        -- virtual environment - the isolated implementation
            --- a 'venv' is a self-contained copy of a Python environment 
                ---- it has its own interpreter binary (venv/bin/python)
                ---- it maintains its own 'site-packages' directory (where packages are installed) 
                ---- it "inherits" the standard library from the system Python but isolates dependencies 
            --- when you activate it, your shell teporarily rewires PATH so 'python' and 'pip' point to that virtual copy, not the system one 


Linux System 

    - binary 

        -- general meaning 
            --- a binary is a file that contains compiled machine code - the 1s and 0s (binary digits) that you CPU can directly execute 
            --- it's called "binary" because its content isn't human-readable text - it's stored in binary (base-2) format, representing CPU instructions, memory addresses, and data
        
        -- binary in software terms
            --- code in high-level languages like Python or C, computer can't run that directly; it must be turned into something your processor understands
                ---- for C/C++, that's done by a compiler, which produces a binary executable (e.g., 'python', 'bash', 'ls')
                ---- for Python, the interpreter itself is a binary - written in C, compiled into an executable file 
            --- so /usr/bin/python3 
                ---- that's the Python interpreter binary - an executable machine-code program that runs on your CPU and knows how to 
                        -> read.py text file 
                        -> parse them into bytecode 
                        -> execute that bytecode line by line 
        
        -- what the /bin folder means 
            --- bin is short for binaries - it's where executable programs live 

            --- typical Linux hierarchy 
                /bin/           → essential system binaries (e.g., bash, ls, cat)
                /usr/bin/       → user-level binaries (e.g., python3, git)
                /usr/local/bin/ → locally installed binaries

            --- in virtual environment
                venv/
                ├── bin/
                │   ├── python       ← the Python binary (executable)
                │   ├── pip          ← the pip command (script wrapper)
                │   ├── activate     ← shell script (text file)
                │   └── ... 

    - Spark Compatibility: why is Windows unstable 

        -- gist: Spark was designed and battle-tested on Unix-like systems; on Windows, several low-level mismatches bite - especially inside partitions where Python workers and JVM executors churn hard 
        
        -- process model & signals
            --- Spark executors launch many short-lived Python workers; on Linux this leans on POSIX semantics (fork/exec, signal delivery, process groups)
            --- Windows lacks POSIX signals/fork; process startup is heavier; termination/cleanup is less reliable
                ---- orphaned workers, slow teardown, weird "point in use" errors, and falky retry behavior during 'mapPartitions'
        
        -- networking & ports 
            --- Python workers talk to the JVM over local sockets 
            --- Windows firewalls/AV, IPv6/IPv4 quirks, and ephemeral-port exhaustion show up more often 
            --- Linux networking stacks handle these high-churn localhost sockets more preditably 
        
        -- filesystem/path semantics 
            --- Spark/Hadoop routines assume POSIX paths, symlinks, and premissive temp-dir behavior 
            --- Windows path rules (backslashes, drive letters, MAX_PATH surprises, case-insensitivity) cause random "file not found" or staging failures mid-task 
        
        -- shell/bootstrap scripts 
            --- Spark/Hadoop ship tons of Bash scripts 
            --- on Windows they run via workarounds (e.g., 'cmd', Cygwin-like layers)
            --- edge cases (CRLF vs LF line endings, environment expansion, quoting) can break executor setup under load
        
        -- Hadoop/winutils baggage
            --- historically you needed 'winutils.exe' and special envs to emulate *nix features 
            --- even with newer Spark, some code paths still assume Unixy behavior - showing up as flaky IO/permissions during shuffle and temp work 
        
        -- Python/native deps 
            --- in partitions you often pull in 'pyarrow', 'pandas', 'numpy' (MKL/OpenBLAS), etc. 
            --- native wheels and memory-mapping behave more consistently on Linux 
            --- on Windows, you'll hit DLL resolution, file-locking, and shared-memory quirks more often under concurrency
        
        -- Antivirums & Defender 
            --- real-time scanning on %TEMP%/ project dirs can stall or lock temporary files that Spark shuffles and deletes aggressively in partition tasks 

    - POSIX 

    - critical files 

        -- '~/.bashrc' - full name: bash run commands

            --- user-level startup script automatically executed every time a new interactive, non-login shell starts (personal shell initialization file for 'bash')

            --- critical functions 

                ---- defines shell environment 
                        -> set up 
                            -> environment variables (PATH, JAVA_HOME, PYTHONPATH, etc.)
                            -> aliases (alias ll='ls -alF')
                            -> functions (custom utilities or wrappers)
                            -> shell options (e.g., 'set -o vi', 'shopt -s histappend')
                        -> so when type 'java', 'python', or 'poetry', whether the system finds them depends on what's in PATH - which is often modified isnide '.bashrc'
                
                ---- controls tool behavior 
                        -> many tools look for environment variables that are set in '.bashrc'
                            --> PySpark reads $JAVA_HOME 
                            --> Poetry or pyenv needs $PATH configured 
                            --> CUDA, conda, or virtualenv setups rely on PATH and LD_LIBRARY_PATH
                            --> prompt customization ('PS1'), history control, etc. 
                
                ---- user-specific customization 
                        -> '~/.bashrc' affects only your user, not the system globally 
                            --> global configs live in '/etc/bash/bashrc' or '/etc/profile'
                            --> changes here are safe for your user and don't affect others 
                
                ---- loaded automatically 
                        -> it's executed automatically for every new interactive shell 
                        -> so, unlike editing '/etc/environment' or '/etc/profile', you don't have to restart your computer or session - just 'source ~/.bashrc'
                
            --- typical lifecycle
                ---- open a new terminal -> bash starts 
                ---- bash loads global configs ('/etc/profile', '/etc/bash.bashrc')
                ---- bash loads your user configs ('~/.bashrc')
                ---- all the exports, aliases, and functions become available 


packaging 

    - pre-requisite 

        -- repo structure 

            spark-etl/
            ├── pyproject.toml
            ├── src/
            │   └── qbo_etl/
            │       ├── __init__.py
            │       ├── pl/
            │       │   ├── __init__.py
            │       │   ├── extract.py
            │       │   ├── flatten.py
            │       │   └── business_rules.py
            │       ├── gl/
            │       │   ├── __init__.py
            │       │   └── ...
            │       └── hr/
            │           ├── __init__.py
            │           └── ...
            ├── notebooks/
            └── config/
        
        -- example pyproject.toml file 

            [project]
            name = "spark-etl"
            version = "0.1.0"
            description = "Spark refactor experimentation for ETL in Python and Pandas"
            authors = [
                {name = "ZheRao",email = "zheraoeminem@gmail.com"}
            ]
            #readme = "README.md"
            requires-python = ">=3.11"
            dependencies = [
                "pyspark (==3.5.1)",
                "pyarrow (>=22.0.0,<23.0.0)",
                "pandas (>=2.3.3,<3.0.0)",
                "orjson (>=3.11.4,<4.0.0)",
                "numpy (>=2.3.4,<3.0.0)",
                "requests (>=2.32.5,<3.0.0)",
                "intuit-oauth (>=1.2.6,<2.0.0)"
            ]


            [build-system]
            requires = ["poetry-core>=2.0.0,<3.0.0"]
            build-backend = "poetry.core.masonry.api"

            [tool.poetry]
            packages = [
            { include = "qbo_etl", from = "src" }
            ]


            [dependency-groups]
            dev = [
                "ipykernel (>=7.1.0,<8.0.0)"
            ]
    
    - 'poetry install'

        -- what it cares about 
            --- 'pyproject.toml' being valid 
            --- dependencies resolving 
            --- build backend existing ('poetry-core' in this case)
        
        -- example output interpretation 

            --- "Installing dependencies from lock file"
                ---- Poetry looked at 'poetry.lock' and your '[projecct]/[dependency-groups]' in pyproject.toml 
                ---- it compared what's already installed in the virtualenv vs what the lock file says 
            
            --- "No dependencies to install or update"
                ---- the env already has all the dependencies, nothing new to do here 
            
            --- "Installing the current project: spark-etl (0.1.0)"

                ---- poetry "installed" your project itself (the thing named 'spark-etl' in [project])

                ---- practically, this is similar in spirit to 'pip install -e .'
                        -> it registers your project in the venv so that tools that look for installed distributions see 'spark-etl==0.1.0'
                        -> whether your 'qbo_etl' package is actually importable depends on how packaging is configured 
        
        -- so poetry achieved 
            --- ensured dependencies match the lock file 
            --- installed your current project as a distribution into the Poetry-managed virtualenv 
    
    - what is a "distribution"

        -- "distribution" means "A build artifact containing your code + metadata, in a standard format that tools like pip can install."

        -- two kinds 
            --- 'sdist' (source distribution): usually a '.tar.gz' with your source 
            --- 'wheel' (built distribution): a '.whl' file, faster to install 
        
        -- when Poetry "installs your project as a distribution", it is doing the equivalent of 
            --- build a wheel for 'spark-etl'
            --- install that wheel into the venv 
        
        -- what is a wheel?

            --- a wheel is just a ZIP archive that follows a specific naming convention and contains 
                ---- your package's code, exactly as it should appear in 'site-packages/'
                ---- metadata (version, dependencies, entry points, etc)
                ---- compiled extensions (if any - C/C++/Rust code, for example)
                ---- a record file that tracks every file in the sheel (for reproducibility)
            
            --- that of a wheel as 
                ---- the final, ready-to-install binary of your Python project 
                ---- like a '.exe' installer for Windows or a '.deb' for Ubuntu - but for Python libraries 
                ---- it is literally the artifact that tools like 'pip' install 
    
    - how other machines can access this package 

        -- important: user has to define the venv first 

            --- inside venv: pip install "git+https://github.com/YourUser/spark-etl.git"

                ---- pip will 
                        -> clone the repo temporarily
                        -> read 'pyproject.toml'
                        -> use Poetry's backend (poetry.core.masonry.api) to build a distribution 
                        -> install that distribution, plus dependencies from '[project].dependencies'
            
            --- alternative flow: full flow 

                    git clone https://github.com/YourUser/spark-etl.git
                    cd spark-etl

                    # create a venv however they like:
                    python -m venv .venv
                    source .venv/bin/activate  # or .venv\Scripts\activate on Windows

                    pip install .
            
                ---- then 
                        -> pip reads your 'pyproject.toml'
                        -> sees 
                                [build-system]
                                requires = ["poetry-core>=2.0.0,<3.0.0"]
                                build-backend = "poetry.core.masonry.api"
                        -> download 'poetry-core' (note: not full Poetry, just the build helper library)
                        -> builds a wheel for spark-etl 
                        -> installs 
                            --> your package ('spark-etl' -> 'qbo_etl' code)
                            --> all dependencies from '[project].dependencies'
                
                ---- note: pip completely ignores 'poetry.lock', it only trusts '[project].dependencies'
                        -> the lock file is primarily for Poetry to reproduce environments 



outside world 

    - web server 




search engine
    
    - Twilio bot - my design
        
        -- goal: find relevant doc by matching user keywords to doc titles

        -- mechanics:

            --- on each query 
                ---- scan all docs (D) 
                ---- for each keyword (K), check if doc contains it 
                ---- build intersection/union sets
                ---- return matches

        -- shape of data 
            
            --- list of docs with small metadata (title + URL)
            --- no persistent state 
            --- search logic: substring match on titles
            --- ranking
                ---- AND-first
                ---- otherwise score by # matched keywords

        -- this is the zero-th version of search
        
    - Enterprise search 
        
        -- this is like 
            
            --- Confluence search
            --- Notion search
            --- internal wiki search
            --- GitHub repo search

        -- key upgrade: inverted index
            --- token -> list of document IDs
            --- this index sits in memory and is reused for thousands of queries 

        -- retrieval pipeline 
            
            1. tokenize query
            2. look up each token in inverted index
            3. combine sets (AND/OR)
            4. rank results using 
                --- term frequency (TF)
                --- inverse document frequency (IDF)
                --- keyword position
                --- keyword proximity
                --- title vs. body scores

        -- why this works
            
            --- fast retrieval O(1) lookup per token
            --- instant scaling to millions of documents
            --- no scanning all documents per query

    - full text search (Elasticsearch, Lucene)
        
        -- this is the real jump - now we index not just titles, but full content

        -- index building becomes - for each document

            --- tokenize -> normalize -> stem -> remove stopwords 
            --- compute term positions
            --- compute TF-IDF weights
            --- store posting lists (token -> ordered list of docs w/ positions)

        -- a posting list entry is something like 
            
                token = "sprayer"
                posting list:
                  doc42: {tf=3, positions=[5,18,30]}
                  doc91: {tf=1, positions=[11]}
                  ...

        -- why positions matter:
            
            --- if query is "wheat sprayer cleanout", and positions are close together, that doc ranks higher 
            --- if keyword appears in title, higher boost 
            --- if keyword appears often, higher TF

        -- query pipline
            
            1. parse tokens
            2. retrieve posting lists
            3. merge posting lists with smart scoring
            4. apply ranking algorithms
            5. serve top results

        -- this is the heart of Lucene, the search library behind
            
            --- Elasticsearch
            --- Solr
            --- many enterprise search engines

    - Google 
        
        -- search engines like Google add many layers on top 

        -- layer 1 - crawling - billions of pages fetched by thousands of crawler machines 

        -- layer 2 - document processing - normalizing
            
            --- text extraction
            --- language detection
            --- boilerplate removal
            --- chunking long pages into segments
            --- deduplication
            --- canonicalization (merge similar URLs)

        -- layer 3 - massive inverted index - same structure as Elasticsearch - but 
            
            --- partitioned across thousands of servers 
            --- replicated
            --- optimized for fast merging
            --- compressed posting lists
            --- stored in memory + SSDs with clever hardware tricks

        -- layer 4 - PageRank (link-based ranking)
            
            --- used to determine "authority" or "importance"
            --- but this is not about keyword matching
            --- it is about global ranking influence 

            --- this is one of the many ranking signals

        -- layer 5 - machine learning ranking models - this is where the action is today
            
            --- BERT-based re-rankers
            --- ColBERT
            --- transformer scoring layers
            --- query-document semantic similarity
            --- click-through signals
            --- personalization signals
            --- "helpfulness" models

            --- the pipeline is 
                
                ---- fast first-stage retrieval - use inverted index to get top 1000 candidates
                ---- slow re-ranking - use ML models to re-score and filter to top 10
                ---- this is where Google uses massive ML inference

        -- layer 6 - serving infrasturcture
            
            --- Google must answer queries in < 200 ms, worldwide, at huge scale

            --- they use 
                
                ---- thousands of replicas
                ---- caching
                ---- geographic routing
                ---- incremental index updates
                ---- specialized hardware
                ---- load balancing



        
                

                


