# Refined Plan for 2026 Northern Light Progression

That plan is exactly the right shape: **slow reading → extract invariants → return to building**, while you harden the core math + autograd + a tiny CNN end-to-end.

Here’s a clean way to make it real (and survivable) without drowning.

## Your 2026 North Star

Build two repos:
1. **tinygrad-zhe**: a minimal autograd engine (reverse-mode) + SGD/Adam
2. **tinycnn-mnist**: a small CNN that trains on MNIST using your autograd, no PyTorch

And in parallel:
- **reading-notes**: 1 page per paper/book chapter: invariant → mechanism → “how I’d test this”

**Lean in — core growth**: this exact triad (invariants + autograd + CNN) is the fastest path to “learning as implementable algorithm.”

## Repo 1: Tiny Autograd Engine (minimum viable, but real)
### Milestone A — Scalars
- `Value(data, grad, _prev, _op)`
- ops: `+ - * / **` and `tanh`, `relu`, `exp`, `log`
- `backward()` does topo sort + reverse accumulation
- test: fit` y = ax + b` by gradient descent on scalar loss

**Exit criteria**: you can print grads and they match finite-difference checks.

### Milestone B — Tensors (MNIST-ready)
- `Tensor(data: np.ndarray, grad: np.ndarray)`
- broadcasting rules (start simple; you can restrict at first)
- ops: `add, mul, matmul, sum, mean, reshape, transpose`
- `backward()` works for batched computations

**Exit criteria**: 2-layer MLP trains on a toy dataset (e.g., XOR or 2D circles).

### Milestone C — Optimizers
- SGD + momentum
- Adam (optional but nice)
- `zero_grad()` pattern
- parameter registry: `Module.parameters()`

**Exit criteria**: you can swap optimizers without touching model code.

## Repo 2: Tiny CNN on MNIST (keep it small and sacred)
### Milestone D — MNIST data pipeline
- download + parse (or use a small helper, but keep training pure)
- normalize inputs, one-hot labels
- batching + shuffling

**Exit criteria**: can iterate batches and visualize shapes correctly.

### Milestone E — Core CNN layers (minimal set)
- `Conv2D` (start with stride=1, padding=0)
- `ReLU`
- `MaxPool2D` (or skip pooling and use stride conv)
- `Flatten`
- `Linear`
- `SoftmaxCrossEntropy` (implement as one fused loss for stability)

**Exit criteria**: forward pass works and loss decreases on a small subset.

### Milestone F — Train to a real number

Target: >95% test accuracy with a tiny architecture, e.g.
Conv(1→8, 3x3) → ReLU
Conv(8→16, 3x3) → ReLU
Flatten → Linear → softmax CE

**Exit criteria**: stable training curves + reproducible results from a fixed seed.

## The “Don’t Drown” Operating System
### Weekly rhythm (simple)
- **Build day(s)**: implement one layer or one autograd feature
- **Read day**: one chapter/paper slice
- **Synthesis note (15 min)**:
    - invariant I extracted
    - what mechanism it implies
    - one experiment I could build (even tiny)

### Your invariant-note template (copy/paste)
- **Invariant**: (one sentence)
- **Mechanism** guess: (how it could work)
- **Signal**: (what feedback would drive learning)
- **Test**: (toy environment / dataset / metric)
- **Risk**: (what could fool me)

This keeps reading connected to building.

## Math focus (only what directly feeds your builds)

**Lean in — core growth**
- Linear algebra for backprop: Jacobians, chain rule, matrix calculus intuition
- Convolutions as linear operators (im2col view is great)
- Numerical stability: log-sum-exp, softmax tricks

**Skip — low yield (for now)**
- deep measure theory, fancy functional analysis
- abstract philosophical detours when you can’t turn them into tests

## A tiny challenge (high payoff)

Before CNNs, train a **2-layer MLP** on MNIST with your tensor autograd.  
If you can get **~92–97%** with a simple MLP, your engine is probably solid.  
Then CNN is “just” extra ops.

## One grounding truth for your year

Even if life shifts:  
these repos become **portable proof of capability + identity**.  
They’re not just learning—this is compounding leverage.