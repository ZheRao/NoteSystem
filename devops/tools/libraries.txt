
python system
    
    - iterable

        -- list related
        
            --- .item() - get a single number out of a list/tensor as a python regular int/float
            
            --- range(start, stop, step)
            
            --- set(list) - create a set object that contains unique values in the list
            
            --- sorted(list) - create a list object that contains sorted values in the list
            
            --- itemgetter(*index_list)(list) - retrieve multiple items from a list with a list of index, operator module
            
            --- list(map(function, target)) - apply the function on elements of target
                ---- map takes all objects in a list and applies a function 
                ---- filter does the same, but takes the results and creates a new list with only the true values (when the function is conditional with if statement)
            
            --- list.index(element, start, end) - locate index of element from start to end positions
            
            --- list.remove("item")
            
            --- list(set(mylist))        - remove duplicated values
        
        -- set related 
            
            --- set_a.add(6)
            
            --- b = next(iter(set_a))                           -----------> accessing the first element of set_a 
            
            --- set_a.pop()                                     -----------> remove the first element from the set 
            
            --- set_a.remove(2)                                 -----------> remove 2 from the set, raise error if 2 is not an element 
            
            --- set_a.discard(2)                                -----------> remove 2 from the set, doesn't raise error if 2 is not an element 
            
            --- set_a.union(set_b) or equivalently set_a | set_b ----> sorted distinct values for both sets
            
            --- set_a.intersection(set_b) or set_a & set_b      -----> common values in both sets 
            
            --- set_a.difference(set_b) or set_a - set_b        -----> values in a not in b 
            
            --- set_a.symmetric_difference(set_b) or set_a ^ set_b -----> values appear only once in both sets 
            
            --- sets cannot be indexed
        
        -- Dictionary
        
            --- for key in dict or for key in dict.keys(): - retrieve the keys
            
            --- list(dict[key].values()):   - convert all values for that particular key in the dictionary to a list
            
            --- for item in dict.items(): - retrieve the entire pairs, e.g., (key1, item1)
            
            --- dict.get(key,alt_value) - retrieve value from key, if not found, return alt_value
            
            --- itemgetter(*keys)(dict) - retrieve items corresponding to the keys
            
            --- with open(path, "wb") as f: pickle.dump(dict,f) - save Dictionary
            
            --- with open(path, "rb") as f: dict=pickle.load(f) - load Dictionary
            
            --- max(stats, key=dict.get) - retrieve the value where the key is the largest
            
            --- sorted(dict.items(),key=lambda x: x[1], reverse=True) - sort dictionary based on value
            
            --- a.update(b) - equivalent to a.extend(b) for lists, this is for dictionary
            
            --- with open(path,"w") as file:
                json.dump(dictfile, file, indent=4)
            
            --- inv_map = {v: k for k, v in my_map.items()}  - inverse dictionary
            
            --- create new dict based on base dict 
                ---- d1 = base | {"new", new1}
                ---- d2 = base | {"new", new2}
            
            --- create new dict from list with default value 
                ---- items = [...]
                ---- d = dict.fromkeys(items, 0)     # default value 0

        -- iterator 
    
            --- use iter() to create an iterator 
                ---- mytuple = ("apple", "banana", "cherry")
                ---- myit = iter(mytuple)
            
            --- use next() method to access the next item in the iterator 
                ---- next_item = next(myit)
            
            --- enumerate(iterable, start)
                ---- takes a collection (e.g., a tuple) and returns it as an enumerate object, it adds a counter as the key of the enumerate object 

    - IO

        -- file-related 

            --- orjson - read raw json bytes 

                ---- 'data_orjson = orjson.loads(raw_bytes)'
    
    - utilities 

        -- logging 

            --- 0. one-sentence truth
                
                ---- Python logging is a name-based, hierarchical message routing system where log records propafate upward until they meet handlers

            --- 1. what a logger is
                
                ---- what it is 
                        -> a named singleton object return by 'logging.getLogger(name)'
                        -> identity is only the string name
                        -> same name -> same logger object

                ---- what it is not
                        -> not tied to files
                        -> not tied to modules
                        -> not passed around (usually)
                        -> not ordered by creation time

            --- 2. logger hierarchy - how "parent/child" works
                
                ---- there is no physical tree in your code 

                ---- hierarchy is built purely by string prefixes split on '.'
                        
                        "sandbox.partition.3"
                        └── parent: "sandbox.partition"
                            └── parent: "sandbox"
                                └── parent: root
                
                ---- Python determines parents by repeatedly removing the last segment

            --- 3. propagation
                
                ---- what happens when you call 'logger.info(...)'
                        -> 1. logger creates a LogRecord
                        -> 2. logger sends record to its own handlers
                        -> 3. if 'propagate=True' (default)
                            --> record is sent to parent logger 
                        -> 4. repeat until root or 'propagate=False'

            --- 4. root logger
                
                ---- 'logging.getLogger()' (not name) returns root 
                ---- root is the final catch-all
                ---- if you configure root handlers, all loggers propafate into it unless stopped
                        -> that's why 'logger = logging.getLogger(__name__)' "just works" once root is configured

            --- 5. handlers (what actually writes logs)
                
                ---- "FileHandler", "StreamHandler", etc.
                ---- attached to specific loggers
                ---- decide 
                        -> where logs go
                        -> how they are formatted
                ---- can be attached anywhere in the hierarchy

                ---- important consequence
                        -> two loggers writing to the same handler -> same destination





            






spark 

 

    - syntax general 

        -- imports 
            --- from pyspark.sql import functions as F 
            --- from pyspark.sql import Window

        -- core DataFrame methods (transformations/actions)

            --- creation & inspection 

                ---- 'spark.read' (csv, json, parquet, jdbc, etc.), 'toDF', 'createDataFrame'

                ---- 'printSchema()', 'dtypes', 'schema', 'columns', 'head()', 'show()', 'limit()'

                ---- 'describe()', 'summary()'
            
            --- column & projection (master now)

                ---- 'select', 'selectExpr'

                ---- 'withColumn', 'withColumns', 'withColumnRenamed', 'alias'

                ---- 'drop', 'dropDuplicates', 'distinct'
            
            --- row filtering 

                ---- 'filter', 'where' (aliases). use column expression or SQL strings 
            
            --- aggregation & grouping 

                ---- 'grouopBy', 'rollup', 'cube' - returns 'GroupedData' with 'agg', 'sum', 'avg', 'count', etc.
            
            --- joins & set operations (master now)

                ---- 'join(other, on=..., how=...)' - 'inter', 'left', 'right', 'outer', 'semi', 'anti', 'cross'

                ---- 'union', 'unionByName', 'intersect', 'exceptAll'
            
            --- ordering, repartitioning & sampling 

                ---- 'orderBy', 'sort'

                ---- 'repartition', 'coalesce', 'repartitionByRange'

                ---- 'sample(withReplacement, fraction, seed=None)', 'randomSplit'
            
            --- missing data 

                ---- 'na.fill', 'na.drop', 'na.replace' 
            
            --- persistence & caching 

                ---- 'cache', 'persist', 'unpersist'

                ---- 'checkpoint(eager=False)'
            
            --- IO (write path)
                
                ---- 'write' -> '.mode()', '.partitionBy()', '.bucketBy()', '.sortBy()', 'format()'
            
        -- column expressions & selectors 

            --- basic column ops 

                ---- column construction: 'F.col', 'F.lit', 'F.expr'

                ---- arithmetic: '+ - * / %', 'F.pow'

                ---- comparison: '==', '!=', '<', '>', '<=', '>='

                ---- boolean: '&', '|', '~' (and, or, not) - remember parenthesis becaose of Python precedence 
            
            --- condition / CASE WHEN (master now)

                ---- 'F.when(cond, value).otherwise(value2)' (SQL 'CASE WHEN')

                ---- 'F.coalesce', 'F.ifnull', 'F.nullif'
            
            --- predicates 

                ---- 'isNull', 'isNotNull', 'between', 'isin', 'like', 'rlike'
        
        -- aggregation functions 

            --- core aggregations 

                ---- 'F.count', 'F.countDistinct'

                ---- 'F.sum', 'F.avg', 'F.min', 'F.max'

                ---- 'F.first', 'F.last'

                ---- 'F.approx_count_distinct', 'F.approx_percentile'
            
            --- stats-ish

                ---- 'F.stddev', 'F.stddev_pop', 'F.stddev_samp'

                ---- 'F.var_pop', 'F.var_samp'

                ---- 'F.corr', 'F.covar_pip', 'F.covar_samp'
            
            --- advnaced grouping (master next)

                ---- grouping sets: groupBy().agg(...) with SQL 'GROUPING SETS / ROLLUP / CUBE' via SQL or expressions 
        
        -- str functions 
                
            --- case: 'F.lower', 'F.upper', 'F.initcap'

            --- trimming: 'F.trim', 'F.ltrim', 'F.rtrim'

            --- length & substring: 'F.length', 'F.substr', 'F.slice' (on arrays or strings)

            --- replacement: 'F.regexp_replace', 'F.translate'

            --- pattern search: 'F.regexp_extract', 'rlike', 'like'

            --- concatenation: 'F.concat', 'F.concat_ws'

            --- padding: 'F.lpad', 'F.rpad'

            --- whitespace normalization / special: 'F.format_string', 'F.soundex', etc.
        
        -- date & timestamp functions 

            --- current time: 'F.current_date', 'F.current_timestamp'

            --- extraction: 'F.year', 'F.month', 'F.dayofmonth', 'F.dayofweek', 'F.weekofyear', 'F.quarter', 'F.date_format'

            --- arithmetic 

                ---- 'F.to_date', 'F.to_timestamp', 'F.unix_timestamp', 'F.from_unixtime'
            
            --- difference 

                ---- 'F.datediff', 'F.months_between'
        
        -- collection / array / map / struct functions (master now)

            --- arrays 

                ---- construction & basic: 'F.array', 'F.size', 'F.array_distinct', 'F.array_contains'

                ---- transformations: 'F.array_union', 'F.array_except', 'F.array_intersect', 'F.sort_array', 'F.array_move'

                ---- zipping & flattening: 'F.arrays_zip', 'F.flatten'

                ---- slicing: 'F.slice', 'F.element_at', 'F.sequence', 'F.shuffle'
            
            --- maps 

                ---- construction: 'F.create_map'

                ---- access & manipulation: 'F.map_keys', 'F.map_values', 'F.element_at', 'F.map_from_entries', 'F.map_entries'
            
            --- structs 

                ---- 'F.struct' to bundle multiple columns into one 

                ---- 'col("struct_col.field")' or 'df.struct_col.field' to access 
        
        -- higher-order functions (arrays/maps)

            --- on arrays/maps, you get: (park - high future value; super powerful once you're comfy with lambdas)

                ---- 'F.transform(array_col, lambda x:...)'

                ---- 'F.filter(array_col, lambda x: condition)'

                ---- 'F.aggregate(array_col, zero, lambda acc, x: ...)'

                ---- 'F.exists', 'F.forall', 'F.reduce', etc. 
            
                These let you express per-element logic in "pure column land" instead of UDFs
        
        -- window functions (master now)

            --- define a window:

                    w = Window.partitionBy("key").orderBy("timestamp")

            --- ranking / row navigation 

                ---- 'F.row_number', 'F.rank', 'F.dense_rank', 'F.ntile'

                ---- 'F.lag', 'F.lead'

                ---- 'F.first', 'F.last', 'F.nth_value' over a window
            
            --- aggregations over a frame 

                ---- 'sum', 'avg', 'min', 'max', 'count' over 'w.rowsBetween(...)', 'w.rangeBetween(...)'
            
            great for time-series, "previous value" logic, cumulative metrics, and advanced QC 
        
        -- JSON / CSV / complex data 

            --- JSON 

                ---- 'F.from_json(col, schema)' - parse JSON string into struct/array 

                ---- 'F.to_json(col)' - struct/array --> JSON string 

                ---- 'F.get_json_object(col, '$.path')'

                ---- 'F.json_tuple(col, 'path1', 'path2')' (SQL-style extraction)
            
            --- CSV 

                ---- 'F.from_csv(col, schema, options)', 'F.to_csv(col, options)'
        
        -- math & numeric utilities 

            --- general math 

                ---- basc: 'F.abs', 'F.round', 'F.bround', 'F.floor', 'F.ceil', 'F.rint'

                ---- exponential & log: 'F.exp', 'F.expm1', 'F.log', 'F.log10', 'F.log2', 'log1p'

                ---- Trig: 'F.sin', 'F.cos', 'F.tan', 'F.asin', 'F.acos', 'F.atan', 'F.atan2' 

                ---- bit operations: 'F.shiftleft', 'F.shiftright', 'F.bit_and', 'F.bit_or', 'F.bit_xor'
            
            --- utility 

                ---- 'F.greatest', 'F.least'

                ---- 'F.rand', 'f.randn' (random numbers)

                ---- formatting: 'F.format_number'
        
        -- null & conditional helpers 

            --- 'F.coalesce(c1,c2, ...)' - first non-null 

            --- 'F.nvl', 'F.nvl2' (in SQL dialects)

            --- 'F.when(...).otherwise(...)' - multi-branch logic 

            --- 'F.nullif(col1, col2)' - null out when equal 

            --- 'F.nanvl' - replace NaN with another value (floating)
        
        -- misc / advanced 

            --- hashing & IDs 

                ---- 'F.md5', 'F.sha1', 'F.sha2', 'F.crc32'

                ---- 'F.monotonically_increasing_id', 'F.uuid'
            
            --- partition transformations & TVFs (newer Spark)

                ---- table arguments: 'df.asTable().partitionBy(...).orderBy(...)' for table-valued functions / UDTFs

                ---- partition transform functions (like 'years()', 'months()') when writing partitioned tables via SQL 
            
            --- UDFs

                ---- row-wise: 'F.udf(py_func, returnType)' - but avoid when possible (slow, no catalyst optimization)

                ---- Pandas/vectorized: 'pandas_udf' (various types: scalar, grouped_agg, map)

                ---- SQL registration: 'spark.udf.register("name", func, returnType)'


                


    - syntax details

        -- IO 

            --- write out 

                (
                    df.repartition(8, "fiscal_year")      # 8 output partitions total, keyed by fiscal_year
                    .write
                    .format("parquet")
                    .mode("append")                       # or "overwrite"
                    .option("compression", "snappy")      # default for Parquet, but explicit is nice
                    .partitionBy("fiscal_year")           # can add more: "company", ...
                    .save("/data/gold/pl")                # instead of .parquet(...)
                )
            
                ---- produce layout 

                    /data/gold/pl/
                        fiscal_year=2023/part-0000-...
                        fiscal_year=2024/part-0000-...
                        fiscal_year=2025/part-0000-...
                
                ---- options 
                        
                        -> '.mode()'
                            --> "error" / "errorifexists" (default): blow up if path exists 
                            --> "overwrite": replacing existing data at the path 
                            --> "append": add new files/partitions 
                            --> "ignore": do nothing if path exists 
                        
                        -> '.repartition("fiscal_year")'
                            --> ensures rows for the same fiscal_year tend to land in the same partition on disk 
                            --> helps avoid having too many tiny files scattered across partitions
                
                ---- important 

                        -> 'spark.conf.set("spark.sql.sources.partitionOverwriteMode", "dynamic")'
                            --> only the partitions present in df (e.g., fiscal_year = 2025) will be overwritten, others stay untouched 
        
        -- 'withColumn'

            --- datatype casting

                ---- 'df = df.withColumn("col_new", F.col("col_old").cast("date"))'
                        -> types: "string", "int"/"integer", "long", "double", "float", "boolean", "date", "timestamp", "decimal(10, 2)" 
                
                ---- specifically parsing non-standard formats - for string not in ISO format
                        -> 'df = df.withColumn("date", F.to_date("raw_date", "yyyy-MM-dd"))'
                        -> 'df = df.withColumn("ts", F.to_timestamp("raw_ts", "yyyy-MM-dd HH:mm:ss"))'
                






 Pandas

    - to force pandas not show scientific notation 
        -- pd.set_option('display.float_format', '{:.2f}'.format)

    - data frame creation
        -- df = pd.DataFrame(data = [[1,2,3], [4,5,6]], columns=["col1", "col2", "col3"], index = ["row1", "row2"])
        -- df = pd.read_csv(path, dtype={"col":"string"}, usecols=columns_to_read)
        -- df = pd.read_parquet(path,)  ------> more storage efficient type
        -- df = pd.read_excel(path, dtype={"col":"string"} sheet_name = "sheet1")
        -- df = pd.read_json(path, dtype={"col":"string"})
        -- empty_df = df.head(0).copy(deep=True)        # keeps all the columns of df, but with empty rows
            --- insert one row 
                    row = {"col1": v1, "col2": v2, ...}
                    empty_df.loc[len(empty_df)] = row

    - inspecting data frame 
        -- df.columns / list(df.index)
        -- df.info() / df.describe()
        -- df.shape / df.size
        -- df.head(n) / df.tail(n)
        -- df.sample(n, random_state=seed_number)

    - querying data 

        -- df["col1"].unique()

        -- accessing columns/items
            --- df.iloc[:,1:6] / df.loc[:,["col1", "col2"]]
            --- df.col1 / df["col1"]
            --- df.at[0,"col1"] -----> more efficient single access

        -- conditioning
            --- df.loc[df["sales"]>=5000, ["itemID","sales"]]
            --- df[df["sales"]>=5000][["itemID","sales"]]
            --- df[(df["sales"]>=5000) & (df["cost"]==30)]
            --- df[df.duplicated(subset=["col1", "col2"], keep="first")]

        -- alternative conditioning
            --- df.query(' country=="USA" and state=="New York" ')

        -- string related conditioning
            --- df[df["name"].str.contains("fire",case=False,regex=False,na=False)]
                ----- regex rules
                        -> anchors -> '^' start of string, '$' end of string 
                            --> e.g., s.str.contains(r"^\d{4}-\d{2}-\d{2}$", na=False)  # exactly a YYYY-MM-DD string
                        -> word boundaries -> '\b', '\B' is the opposite (no boundries)
                            --> e.g., \bhello\b will match "hello world" or "say hello!", but won't match "shellos"
                        -> character classes
                            --> [abc] any of a, b, or c 
                            --> [A-Za-z]
                            --> [^0-9] not a digit
                        -> predefined classes 
                            --> '\d' digit 
                            --> '\w' word char (letters/digits/underscore)
                            --> '\s' whitespace
                            --> uppercase means the inverse 
                        -> quantifiers: how many times the preceding "token" (a character, a group, ...) should be repeated 
                            --> '*'; 0 or more; e.g., "ab*c" will match "ac", "abc", "abbbbbc"
                            --> '+'; 1 or more; e.g., "ab+c" will not match "ac"
                            --> '?'; 0 or 1 (optional, when ? is used as quantifier); e.g., "colou?r" will match "color" or "colour"
                            --> '{m}'; exactly m matches; e.g., \d{3} matches exactly three digits
                            --> '{m,}'; m or more; 
                            --> '{m,n}'; between m nad n occurrences (inclusive)  
                        -> groups 
                            --> (...) capturing - matches can be referenced back 
                            --> (?:...) non capturing - matches cannot be reference back 
                        -> lookarounds: zero-width assertions
                            --> (?=...) must be followed by ...
                                ---> 'A(?=B)'; positive lookahead; match A only if it is followed by B, B is not part of what's matched 
                            --> (?!...) must not be followed by ...
                                ---> 'A(?!B)'; negative lookahead; match A only if not followed by B 
                            --> (?<=...) must be preceded by ...
                                ---> '(?<=B)A'; positive lookbehind; match A only if it is preceded by B 
                            --> (?<!...) must not be preceded by ...
                                ---> '(?<!B)A'; negative lookbehind; match A only if not preceded by B 
                            --> zero-width assertion means after completing the assessment, the search position is still at the same place as when it started the lookarounds search 
                ---- regex examples 
                        ->
            
            --- .str.startwith()

        -- .isin 
            --- df[df["country"].isin(["USA", "UK"])]
        
        -- .isna() / .notna()
            --- df[~df["col1"].isna()]
        
    - modifying data

        -- df.sort_values(["col1","col2"], ascending=[False,True]) ------> sort col2 by asscending order, then col1 by descending order 

        -- modifying column values
            --- df.iloc[1,["col1"]] = value 
            --- df["price"] = np.where(df["item_type"] == "expensive", 999.99, 1.99)
            --- df["total_price"] = df["units_sold"] * df["cost"]
            --- df.drop_duplicates(subset=["col1", "col2"], ignore_index=)             --------------> ignore_index=True will reset index
            --- df["col_min"] = df[df["col1", "col2", "col3"]].idxmin(axis=1)          --------------> return the column index with the minimum value among the columns
            --- df = df.astype({"col1": type1, "col2":type2})
            --- df["Amount"] = df["Amount"].clip(lower=0)   
                ---- vectorized method for clipping any value below 0 (with 'lower=0') or any value above 0 (with 'upper=0') as 0, and keep missing values 

            --- converting column to date format
                    df["date"] = pd.to_datetime(df["date"], format="%Y-%m-%d", errors="coerce")

                        -> "09/07/2025 04:31 PM" is month/day/year with a 12-hour clock + AM/PM. Use %m/%d/%Y %I:%M %p (note %I = 1–12 hour, %p = AM/PM)
                        -> Milliseconds: "2025-09-24 13:47:05.123" → "%Y-%m-%d %H:%M:%S.%f"
                        -> “T” separator: "2025-09-24T13:47:05" → "%Y-%m-%dT%H:%M:%S"
                        -> Timezone offset: "2025-09-24 13:47:05+00:00" → include %z: "%Y-%m-%d %H:%M:%S%z" (use %Z for names like UTC)
                        -> errors="coerce" sets bad rows to NaT

                    df["date diff"] = pd.to_timedelta(df["date diff"])
                    df["days_away_from_Monday"] = pd.to_timedelta(df["date"].dt.weekday, unit="D")
                    df["date + 1"] = df["date"] + pd.Timedelta(days=1)
                        entry + dt.timedelta(days=1)    # for a single row processing 

            --- string related
                ---- df["first_name_only"] = df["name"].str.split(" ").str[0] - vectorized
                ---- df["col"].str.zfill(2) - take two string digit from a column, if there's just a single digit, pad the front with '0'

            --- take only year
                    df["year"] = df["date"].dt.year
                    
            --- take month names 
                    df["Month"] = df["date"].dt.month_name()
        
        -- modifying columns 
            --- df = df.drop(columns = ["col1", "col2"])
            --- df = df.rename(columns={"old_col_name": "new_col_name"})
            --- df.add_prefix("col_")                                                ------------------------> add "col_" to the beginning of every column name
            --- df["str_col"] = df["str_col"].replace({"old_val": "new_val", "old_val2":"new_val2"})
            --- df["str_col"] = df["str_col"].str.slice(3,5)                            ---> not including 5th char
                                             .str.contains()
                                             .str.startswith()

        -- copy data frame 
            --- df2 = df1.copy(deep=True)
        
        -- merging and concatenating 
            --- df3 = pd.merge(df1, df2, on="col_both", how="left", suffix=["df1","df2"])
            --- df3 = pd.concat([df1, df2], ignore_index=True)          ------> create a new index column for the combined dataframe
        
        -- set index column
            --- df.set_index("column_name")  --------------> set the column as index 
            --- df.reset_index(drop=)        --------------> create a default index column, drop=True will remove the current index column
        
        -- assess whether a column value is between intervals
            --- a = pd.IntervalIndex.from_arrays(left = intervals["left"], right = intervals["right"], closed = "both") ---> create [value1_left, value1_right], ... boundries from intervals df 
            --- a.get_indexer([col_val1, col_val2, ...]) will return an array of positions that align the column values with the index boundary 'a', and -1 if no match found
        
        -- unpivot 
            --- flatten_df = df.melt(
                id_vars = ["location", "currency"],     # columns that should be repeated for each flattened row 
                var_name = "Commodity",                 # column values that should be flattened from horizonal to vertical, e.g., in the pivot table, commodities are the columns 
                value_name = "Amount"                   # column name for holding the actual values  
            )
        
        -- expanding composite column values into separate rows while replicating other columns 
            --- df = pd.DataFrame({
                    'ID': [1, 2],
                    'Combined': ['value1+value2+value3', 'foo+bar+baz'],
                    'OtherCol': ['A', 'B']
                })

                df['Combined'] = df['Combined'].str.split('+')
                        | ID | Combined                 | OtherCol |
                        | -- | --------                 | -------- |
                        | 1  | [value1,value2,value3]   | A        |
                        | 2  | [foo, bar, baz]          | B        |

                df_exploded = df.explode('Combined').reset_index(drop=True)
                        | ID | Combined | OtherCol |
                        | -- | -------- | -------- |
                        | 1  | value1   | A        |
                        | 1  | value2   | A        |
                        | 1  | value3   | A        |
                        | 2  | foo      | B        |
                        | 2  | bar      | B        |
                        | 2  | baz      | B        |

    - aggregating 

        -- df["col1"].value_counts()

        -- df.groupby(["col1"]).agg({"col2":"sum", "col3":"mean", "col4": "prod"}).reset_index(drop=False)          # keep the complete information in each row 
            --- vc_map.groupby("TransactionID_partial")["TransactionEntered"].agg(lambda s: " ".join(s.dropna().astype(str))).reset_index()
                ---- for each TransactionID_partial group, concatenate TransactionEntered into one 
                ---- the lambda receives the TransactionEntered Series per group, or use .agg(" ".join) if the data are already strings with no NaNs
                        -> conceptualize as the function inside .agg() will be applied to each column series passed from .groupby per group 
                        -> e.g., {"a" : "sum", "b": "sum"} will apply sum to all the columns values for a per group, and same as b
                        -> specifying column after groupby will enbale more flexibility in function definition inside .agg() because it will only apply to one column series 
                            --> but this will set the column as index unless otherwise specified, so don't use .reset_index(drop=True) later
                ---- alternatively: vc_map.groupby("TransactionID_partial", as_index=False).agg(TransactionEntered_concat=("TransactionEntered", " ".join))

        -- pivot
            --- pivot = df.pivot(columns = "col_for_column", index = "col_for_row", values = "col", aggfunc="sum", fill_value=0)

    - iterations
        -- for index, row in df.iterrows(): ------> memory inefficient
        -- df["col"] = df.apply(lambda x: fun(x), axis=1)
            --- axis=1: iterating through each row 
            --- axis=0: iterating through each column
    
    - NULL values

        -- NA counts
            --- df.isna().sum()
        
        -- fill NA
            --- df = df.fillna(value = {"col1": df["col1"].mean(), "col2": 0, "col3": 400}, limit=20)        -------- replace the first 20 NULL according to the columns 
            --- df["col1"] = df["col1"].interpolate()
            --- fill na by using a mapping: fills only NaN entries in Product with the value from mapping_table at the same index, Non-missing Product values remain unchanged
                s = pd.Series({"val_mapped_from_1":"val_mapped_to_1", ..., "val_mapped_from_n":"val_mapped_to_n"})
                mapping_table = df["col_mapped_from"].map(s)    -   create a equal-length (to original df) series 
                df["Product"] = df["Product"].fillna(mapping_table)
        
        -- drop NA
            --- df = df.dropna(subset = ["col1", "col2"], how="all")         ----> or how = "any" to drop row when any of col1 or col2 is missing
        
    - advanced functionalities

        -- shift - create a copy of a column and shift the values by 1, facilitate calculating percentage change
            --- df["yesterday_revenue"] = df["revenue"].shift(1)

        -- create rank
            --- df["col_rank"] = df["col_compare"].rank(ascending=False)
        
        -- rolling cumulation
            --- df["cumulative_revenue"] = df["revenue"].cumsum()
    
    - optimization in reading csv - after version 2.0
        -- pd.read_csv(path, engine="pyarrow", dtype_backend = "pyarrow")
    
    - write out data
        -- df.to_csv(path,index=False)
        -- writer = pd.ExcelWriter("path_file.xlsx")
            --- df1.to_excel(writer, sheet_name = "sheet_name1", index=False)
            --- df2.to_excel(writer, sheet_name = "sheet_name2", index=False)
            --- writer.close()

    - working with JSON
        -- df = pd.read_json(path)

        -- df_json = df.to_dict(orient="records")       ------ convert dataframe back to JSON format 
            --- orient = "records"
                    [
                        {"col1": value_00, "col2": value_01, ...},
                        {"col1": value_10, "col2": value_11, ...},
                        ...
                    ]
            --- orient = "index"
                    [
                        0: {"col1": value_00, "col2": value_01, ...},
                        1: {"col1": value_10, "col2": value_11, ...},
                        ...
                    ]  
            --- orient = "split"
                    {
                        "index": [0, 1, ...],
                        "columns": ["col1", "col2", ...],
                        "data": [
                            [value_00, value_01, ...],
                            [value_10, value_11, ...],
                            ...
                        ]
                    }
            --- orient = "dict"
                    {
                        "col1": {0: value_00, 1: value_10, ...},
                        "col2": {0: value_01, 1: value_11, ...},
                        ...
                    }
            --- orient = "list"
                    {
                        "col1": [value_00, value_10, ...],
                        "col2": [value_01, value_11, ...],
                        ...
                    }
            --- orient = "series"

        -- df_flatten = pd.json_normalize(df_json, sep=".", max_level = 2)                                                                  
            --- go to each sublevel of a column that is nested dictionary 
        
        -- df_flatten2 = pd.json_normalize(df_json, record_path=["key_treat_as_list_of_records"], meta=["fields_included_for_flatten"], record_prefix="line.") 
            --- treat each item from record_path as individual records (duplicate other fields)
            --- this expansion will keep "line." as prefix for the nested columns























