
python system
    
    - iterable

        -- list related
        
            --- .item() - get a single number out of a list/tensor as a python regular int/float
            
            --- range(start, stop, step)
            
            --- set(list) - create a set object that contains unique values in the list
            
            --- sorted(list) - create a list object that contains sorted values in the list
            
            --- itemgetter(*index_list)(list) - retrieve multiple items from a list with a list of index, operator module
            
            --- list(map(function, target)) - apply the function on elements of target
                ---- map takes all objects in a list and applies a function 
                ---- filter does the same, but takes the results and creates a new list with only the true values (when the function is conditional with if statement)
            
            --- list.index(element, start, end) - locate index of element from start to end positions
            
            --- list.remove("item")
            
            --- list(set(mylist))        - remove duplicated values
        
        -- set related 
            
            --- set_a.add(6)
            
            --- b = next(iter(set_a))                           -----------> accessing the first element of set_a 
            
            --- set_a.pop()                                     -----------> remove the first element from the set 
            
            --- set_a.remove(2)                                 -----------> remove 2 from the set, raise error if 2 is not an element 
            
            --- set_a.discard(2)                                -----------> remove 2 from the set, doesn't raise error if 2 is not an element 
            
            --- set_a.union(set_b) or equivalently set_a | set_b ----> sorted distinct values for both sets
            
            --- set_a.intersection(set_b) or set_a & set_b      -----> common values in both sets 
            
            --- set_a.difference(set_b) or set_a - set_b        -----> values in a not in b 
            
            --- set_a.symmetric_difference(set_b) or set_a ^ set_b -----> values appear only once in both sets 
            
            --- sets cannot be indexed
        
        -- Dictionary
        
            --- for key in dict or for key in dict.keys(): - retrieve the keys
            
            --- list(dict[key].values()):   - convert all values for that particular key in the dictionary to a list
            
            --- for item in dict.items(): - retrieve the entire pairs, e.g., (key1, item1)
            
            --- dict.get(key,alt_value) - retrieve value from key, if not found, return alt_value
            
            --- itemgetter(*keys)(dict) - retrieve items corresponding to the keys
            
            --- with open(path, "wb") as f: pickle.dump(dict,f) - save Dictionary
            
            --- with open(path, "rb") as f: dict=pickle.load(f) - load Dictionary
            
            --- max(stats, key=dict.get) - retrieve the value where the key is the largest
            
            --- sorted(dict.items(),key=lambda x: x[1], reverse=True) - sort dictionary based on value
            
            --- a.update(b) - equivalent to a.extend(b) for lists, this is for dictionary
            
            --- with open(path,"w") as file:
                json.dump(dictfile, file, indent=4)
            
            --- inv_map = {v: k for k, v in my_map.items()}  - inverse dictionary
            
            --- create new dict based on base dict 
                ---- d1 = base | {"new", new1}
                ---- d2 = base | {"new", new2}
            
            --- create new dict from list with default value 
                ---- items = [...]
                ---- d = dict.fromkeys(items, 0)     # default value 0

        -- iterator 
    
            --- use iter() to create an iterator 
                ---- mytuple = ("apple", "banana", "cherry")
                ---- myit = iter(mytuple)
            
            --- use next() method to access the next item in the iterator 
                ---- next_item = next(myit)
            
            --- enumerate(iterable, start)
                ---- takes a collection (e.g., a tuple) and returns it as an enumerate object, it adds a counter as the key of the enumerate object 

    - IO

        -- file-related 

            --- orjson - read raw json bytes 

                ---- 'data_orjson = orjson.loads(raw_bytes)'
    
    - utilities 

        -- logging 

            --- 0. one-sentence truth
                
                ---- Python logging is a name-based, hierarchical message routing system where log records propafate upward until they meet handlers

            --- 1. what a logger is
                
                ---- what it is 
                        -> a named singleton object return by 'logging.getLogger(name)'
                        -> identity is only the string name
                        -> same name -> same logger object

                ---- what it is not
                        -> not tied to files
                        -> not tied to modules
                        -> not passed around (usually)
                        -> not ordered by creation time

            --- 2. logger hierarchy - how "parent/child" works
                
                ---- there is no physical tree in your code 

                ---- hierarchy is built purely by string prefixes split on '.'
                        
                        "sandbox.partition.3"
                        └── parent: "sandbox.partition"
                            └── parent: "sandbox"
                                └── parent: root
                
                ---- Python determines parents by repeatedly removing the last segment

            --- 3. propagation
                
                ---- what happens when you call 'logger.info(...)'
                        -> 1. logger creates a LogRecord
                        -> 2. logger sends record to its own handlers
                        -> 3. if 'propagate=True' (default)
                            --> record is sent to parent logger 
                        -> 4. repeat until root or 'propagate=False'

            --- 4. root logger
                
                ---- 'logging.getLogger()' (not name) returns root 
                ---- root is the final catch-all
                ---- if you configure root handlers, all loggers propafate into it unless stopped
                        -> that's why 'logger = logging.getLogger(__name__)' "just works" once root is configured

            --- 5. handlers (what actually writes logs)
                
                ---- "FileHandler", "StreamHandler", etc.
                ---- attached to specific loggers
                ---- decide 
                        -> where logs go
                        -> how they are formatted
                ---- can be attached anywhere in the hierarchy

                ---- important consequence
                        -> two loggers writing to the same handler -> same destination





            






spark 

 
    - syntax details

        -- IO 

            --- write out 

                (
                    df.repartition(8, "fiscal_year")      # 8 output partitions total, keyed by fiscal_year
                    .write
                    .format("parquet")
                    .mode("append")                       # or "overwrite"
                    .option("compression", "snappy")      # default for Parquet, but explicit is nice
                    .partitionBy("fiscal_year")           # can add more: "company", ...
                    .save("/data/gold/pl")                # instead of .parquet(...)
                )
            
                ---- produce layout 

                    /data/gold/pl/
                        fiscal_year=2023/part-0000-...
                        fiscal_year=2024/part-0000-...
                        fiscal_year=2025/part-0000-...
                
                ---- options 
                        
                        -> '.mode()'
                            --> "error" / "errorifexists" (default): blow up if path exists 
                            --> "overwrite": replacing existing data at the path 
                            --> "append": add new files/partitions 
                            --> "ignore": do nothing if path exists 
                        
                        -> '.repartition("fiscal_year")'
                            --> ensures rows for the same fiscal_year tend to land in the same partition on disk 
                            --> helps avoid having too many tiny files scattered across partitions
                
                ---- important 

                        -> 'spark.conf.set("spark.sql.sources.partitionOverwriteMode", "dynamic")'
                            --> only the partitions present in df (e.g., fiscal_year = 2025) will be overwritten, others stay untouched 
        
        -- 'withColumn'

            --- datatype casting

                ---- 'df = df.withColumn("col_new", F.col("col_old").cast("date"))'
                        -> types: "string", "int"/"integer", "long", "double", "float", "boolean", "date", "timestamp", "decimal(10, 2)" 
                
                ---- specifically parsing non-standard formats - for string not in ISO format
                        -> 'df = df.withColumn("date", F.to_date("raw_date", "yyyy-MM-dd"))'
                        -> 'df = df.withColumn("ts", F.to_timestamp("raw_ts", "yyyy-MM-dd HH:mm:ss"))'
                

























