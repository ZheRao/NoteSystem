





            






spark 

 
    - syntax details

        -- IO 

            --- write out 

                (
                    df.repartition(8, "fiscal_year")      # 8 output partitions total, keyed by fiscal_year
                    .write
                    .format("parquet")
                    .mode("append")                       # or "overwrite"
                    .option("compression", "snappy")      # default for Parquet, but explicit is nice
                    .partitionBy("fiscal_year")           # can add more: "company", ...
                    .save("/data/gold/pl")                # instead of .parquet(...)
                )
            
                ---- produce layout 

                    /data/gold/pl/
                        fiscal_year=2023/part-0000-...
                        fiscal_year=2024/part-0000-...
                        fiscal_year=2025/part-0000-...
                
                ---- options 
                        
                        -> '.mode()'
                            --> "error" / "errorifexists" (default): blow up if path exists 
                            --> "overwrite": replacing existing data at the path 
                            --> "append": add new files/partitions 
                            --> "ignore": do nothing if path exists 
                        
                        -> '.repartition("fiscal_year")'
                            --> ensures rows for the same fiscal_year tend to land in the same partition on disk 
                            --> helps avoid having too many tiny files scattered across partitions
                
                ---- important 

                        -> 'spark.conf.set("spark.sql.sources.partitionOverwriteMode", "dynamic")'
                            --> only the partitions present in df (e.g., fiscal_year = 2025) will be overwritten, others stay untouched 
        
        -- 'withColumn'

            --- datatype casting

                ---- 'df = df.withColumn("col_new", F.col("col_old").cast("date"))'
                        -> types: "string", "int"/"integer", "long", "double", "float", "boolean", "date", "timestamp", "decimal(10, 2)" 
                
                ---- specifically parsing non-standard formats - for string not in ISO format
                        -> 'df = df.withColumn("date", F.to_date("raw_date", "yyyy-MM-dd"))'
                        -> 'df = df.withColumn("ts", F.to_timestamp("raw_ts", "yyyy-MM-dd HH:mm:ss"))'
                

























