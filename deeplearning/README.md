# Deep Learning Note System

## Overview

A layered learning system for mastering modern deep learning foundations, with an ephasis on 
understanding how network structures transform representations and enable learning.

The long-term goal is to experiment with and explore alternative network designs that move closer
to biologically inspired intelligence systems.

## Components

- **Layer0-LinearAlgebra.txt**
    - Intuitive understanding of how neural network operations transform and mutate the geometric space of inputs
    - Important insights so far:
        1. Linear transformations project inputs into different representations (coordinate systems)
        2. Non-linearity introduces irreversible deformation of reprentation space

- **Layer1-Autograd.txt**
    - Hands-on implementation of automatic differentiation and backpropagation
    - Deep understanding of how learning signals propagate through a network

- **Layer2-NNPrimitives.txt**
    - Construct and experiment core architectures (MLP, CNN, RNN)
    - Observing how architectural choices impose inductive bias on loss landscape and training dynamics

- ***NorthernLight.txt***
    - High-level concepts and architectural principles of a next-generation intelligence system
    - Current LLMs assume intelligence emerges from sequence modeling under static identity
    - This project assumes intelligence is a continuously evolving internal state shaped by irreversible experience
