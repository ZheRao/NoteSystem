


Linux environment

    - Ubuntu - start from scratch

        -- start from /home/user 

        -- 'mkdir projects && cd projects' - now inside /home/user/projects 

        -- 'code .' - launch VS Code inside projects folder 
        
    - VS Code - setup WSL env and install necessary packages including python

        -- one-time WSL setup (global tools only)

            --- 'sudo apt update && sudo apt upgrade -y'
                
                ---- keep Ubuntu tidy by updating package list, then upgrade everything that has updates 

                ---- command breakdown 
                        -> 'sudo': run with administrator privileges (needed to change system packages)
                        -> 'apt update': refreshes the local package index from your configured repositories; it doesn't install anything, just learns what the latest versions are 
                        -> 'apt upgrade -y': downloads and installs newer versions of all currently installed packages that have safe upgrades available; '-y' auto-answers "yes"
                
                ---- benefits
                        -> get the latest security patches and bug fixes promptly 
                        -> installed packages stay consistent with repo versions, reducing dependency conflicts
                        -> clears up "stale index" issues (e.g., 404s when repos change) by refreshing metadata first 
                
                ---- further improvements
                        -> doesn't remove unused packages, to clean leftovers: 'sudo apt autoremove' (and optionally 'sudo apt autoclean')
                        -> 'apt upgrade' won't remove packages; if a dependency change requires removals, use 'sudo apt full-upgrade' (more aggressive)
            
            --- 'sudo apt install -y build-essential curl git pkg-config unzip'

                ---- essentials for building python wheels, talking to git, etc. 

                ---- packages 
                        -> build-essential: a meta-package that pulls in GCC/G++ compilers, 'make', and basic headers/tools needed to compile C/C++ extensions
                        -> curl: command-line HTTP client (download files, hit APIs, fetch install scripts)
                        -> git: version control CLI 
                        -> pkg-config: helper that tells compilers where to find headers/libs for dependencies (e.g., '--cflags', '--libs' for 'libssl', 'libxml2', etc.)
                
                ---- "wheels" in Python 
                        -> a wheel ('.whl') is Python's standard built distribution format (PEP 427)
                        -> think of it as a prebuilt, ready-to-install package; when run 'pip install <package>', pip prefers a wheel because it can just unpack files - no compiling
                        -> if a wheel isn't available for your platform/Python version, pip falls back to a source distribution ('sdist', usually a '.tar.gz')
                            and will try to build a wheel locally; that's when you need compilers and headers - hence 'build-essential' and 'pkg-config'
                
                ---- common add-ons when needed (when something fails)
                        -> Python headers: 'sudo apt install -y python3-dev'
                        -> SSL/cryptography: 'sudo apt install -y libssl-dev libffi-dev'
                        -> compression/CSV/Parquet/etc: 'sudo apt install -y zlib1g-dev libbz2-dev liblzma-dev'
                        -> XML: 'sudo apt install -y libxml2-dev libxslt1-dev'
            
            --- 'sudo pat install -y python3 python3-nenv python3-pip'

                ---- packages 
                        -> 'python3': installs Ubuntu's system Python interpreter (usually /usr/bin/python3)
                        -> 'python3-venv': lets you create virtual environment with 'python3 -m venv'
                        -> 'python3-pip': installs pip, Python's package manager 
                
                ---- purpose 
                        -> Ubuntu already uses Python internally (for system tools, package managers, etc.) and that's called the system python 
                            --> if lives under /usr/bin/python3 and is managed by 'apt', not by 'pip'
                        -> it's fine to use this system Python just to bootstrap - that is, to create isolated environments (venv) and install 'pip'
                        -> you should not install your project libraries (like 'pandas', 'torch', etc.) directly into the system python
                            --> after initialization, all real work happens inside the virtual environments, not the system install 
                
                ---- danger of not isolation
                        -> if you 'pip install' or 'pip uninstall' into /usr/lib/python3/dist-packages, you can break the OS 
                        -> it mixes system and user dependencies - hard to maintain 
                            --> you'll lose control over versions when Ubuntu updates packages 

            --- 'sudo apt install -y pipx' and then 'pipx ensurepath' and then (after reboot)

                ---- sets up pipx, a tool for installing Python CLI apps in their own isolated virtual environments - great for "global" tools like Jupyter - without polluting system Python or Conda envs 

                ---- first command - installs the 'pipx' launcher from Ubuntu repos 

                ---- second command - adds pipx's binary directory (typically ~/.local/bin) to your PATH (by updating your shell rc files)
                        -> after this, newly installed pipx apps are runnable by name 
                
                ---- how pipx works 
                        -> when you run 'pipx install <app>', pipx 
                            --> create a dedicated venv under ~/.local/pipx/venvs/<app>/
                            --> installs the app there 
                            --> symlinks the app's console scripts into ~/.local/bin/
                        -> your Python libraries for projects should still live in your project envs (Conda/venv); use pipx only for standalone command-line tools
                
                ---- 'pip install' vs. 'pipx install'
                        -> 'pip install' installs into the currently active Python environment (your Conda env, venv, or - dangerously - system Python) 
                            --> new package will share that env's packages 
                        -> 'pipx install jupyterlab' creates a dedicated isolated venv just for the new package and exposes 'jupyter-lab' command on your PATH 
                            --> your project packages live elsewhere; Jupyter is just the "app shell"
    
            --- global packages 
                ---- 'pipx install poetry'
                ---- 'pipx install --include-deps jupyter'
                        -> by default, pipx only exposes apps from the top-level packages you install 
                        -> many packages are meta packages (they don't ship a CLI themselves; their dependencies do), 'jupyter' meta package is like that 
                        -> 'jupyter' meta package depends on things like 'jupyter-core', but it doesn't have its own app 
                            --> so 'pipx install jupyter' won't expose the dependency apps unless 'pipx install --include-deps jupyter'

    - Java for PySpark (OpenJDK 17)

        -- 'sudo apt install -y openjdk-17-jdk'
            --- installs the Java 17 JDK (compiler + tools) 
            --- PySpark needs a JVM; Java 11 or 17 is typical 
        
        -- echo 'export JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64' >> ~/.bashrc
            --- appends a line to your '~/.bashrc' setting JAVA_HOME to where OpenJDK 17 lives (on Ubuntu this path is standard)
            --- many tools (including PySpark) look for JAVA_HOME 
            --- 'export' -> marks "JAVA_HOME" variable as an environment variable - visible to child processes (commands or programs launched from this shell)

        -- echo 'export PATH="$JAVA_HOME/bin:$PATH"' >> ~/.bashrc 
            --- appends a line that puts Java's 'bin' directory at the front of your PATH so 'java', 'javac', etc. resolve to this JDK 
        
        -- source ~/.bashrc 
            --- reloads your shell configuration so the new variables take effect immediately 
    




git

    - create/activate SSH keys for GitHub

            ssh-keygen -t ed25519 -C "your_email@example.com"
            eval "$(ssh-agent -s)"
            ssh-add ~/.ssh/id_ed25519
            cat ~/.ssh/id_ed25519.pub
        
        -- copy the printed key -> GitHub -> Settings -> SSH and GPG keys -> New SSH key

    - one-time global setup  




Spark setup 




Twilio setup 



    - login 

        -- 'twilio login'
    
    - comman operations 

        -- create a project 
            --- 'twilio assets:init --service-name sop-docs'

        -- check and activate profile 
            --- 'twilio profiles:list'
            --- 'twilio profiles:use monette-zhe'
        
        -- assets 
            --- 'twilio assets:list'
                ---- 'twilio assets:list -o json > assets/assets_meta.json'
            --- 'twilio assets:upload ./grain_handling.pdf' 

        
        -- upload bulk files 

                cd ~/files
                for f in *.pdf; do
                    twilio assets:upload "$f";
                done
        
        -- nuke assets

            twilio api:serverless:v1:services:remove --sid ZSxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

            --- find sid 

                twilio api:serverless:v1:services:list
            
            --- check inside service for presence of functions (identify code services)

                ---- twilio api:serverless:v1:services:functions:list --service-sid ZS_...

            --- after nuking, delete local cached pointers to the old ZID 

                ---- find the file 
                        grep -R "ZSec9c6a61ec2124949b56da9845c6bbcd" -n ~ 2>/dev/null | head -n 20
                
                ---- look for 
                        ~/.twilio-cli/plugins/@twilio-labs/plugin-assets/config.json
                        ~/path/to/code/whatsapp-backend/assets/assets_meta.private.js
                
                ---- rm both
        
        -- recreate service for asset upload 

            --- twilio assets:init --service-name sop-docs
                note: don't do it inside code directory 

        
        -- install serverless plugin 
            --- 'twilio plugins:install @twilio-labs/plugin-serverless'
        
        -- initialize functions project 

            --- 'twilio serverless:init sop-whatsapp-backend'
            --- 'cd sop-whatsapp-backend'

            --- structure 

                    sop-whatsapp-backend/
                        .env
                        functions/
                            hello-world.js   (sample)
                        assets/
                        package.json

        -- test deploy 
            --- 'cd ~/files/whatsapp-backend'
            --- 'twilio serverless:start'
            --- CTRL+c - cancel
            --- example HTTP request for fake messages 

curl -X POST http://localhost:3000/whatsapp-reply \
    -d "Body=get-chemical, inspection" \
    -d "From=whatsapp:+15551234567" \
    -d "To=whatsapp:+1YOURTWILIONUM" \
    -d "ProfileName=Local Test" \
    -d "NumMedia=0"                
    
        -- actual deploy 
            --- 'cd ~/files/whatsapp-backend'
            --- 'twilio serverless:deploy'
        
        -- logout 
            --- 'twilio logout'


- config


Linux path:
files/

win_path="/mnt/c/Users/ZheRao/OneDrive - Monette Farms/Desktop/Work Files/Projects/6-SOP/final/uploads/"

rsync -av "$win_path" files/

twilio assets:list -o json > assets/assets_meta.private.js



find how many files in directory

ls -1 | wc -l


upload < 100 files at a time













