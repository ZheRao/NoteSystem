




Year 1 - DeepLearning - Foundation: Build Mathematical & Experimental Intuition 

    - goal 
        -- rebuild your neural network understanding from first principles, now with modern engineering rigor 
        -- this year is about mastering the basics throughly - not just knowing what a neural network is, but being able to implement and experiment with them at a low level 

    - additional topics 

        -- PyTorch Internals
            --- why: open the hood a bit - enough to see how the engine breathes, for training custom models 
            --- areas:
                ---- autograd engine fundamentals
                        -> how 'requires_grad', 'backward()', '.detach()' work 
                        ->retain_graph, in-place ops, memory freeing 

    - Q1 - Re-Foundations of Deep Learning (Autograd & Backprop from Scratch)

        -- goal 
            --- focus on fundamental mechanics of neural networks 
            --- ensure you understand automatic differentiation and backpropagation at a granular level 
            --- learn how frameworks like PyTroch perform autograd by building a computational graph and applying the chain rule in reverse to compute gradients 
            --- revisit how weights are initialized and why initialization matters for training stability 

        -- month 1 - The Mathematical Core

            --- outcome: derive and code gradients by hand; focus on understanding and deriving the chain rule for NN 

            --- week 1 - computational graphs & the chain rule 

                ---- study
                        -> study Karpathy's video (into and backprop): https://www.youtube.com/watch?v=VMj-3S1tku0&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&index=2
                        -> read 
                            --> CS231n Backprop Notes at https://cs231n.github.io/optimization-2/ 
                            --> 3Blue1Brown: Neural Networks — The Backpropagation Algorithm
                        -> read (light): Deep Learning Book, Ch. 6.5 Back-propagation and Other Differentiation Algorithms

                ---- practice 
                        -> write out by hand a computational graph for a simple function, e.g., f(x,y)=(x+y)∗y, compute ∂f/∂x and ∂f/∂y step by step
                        -> implement a minimal scalar reverse-mode autodiff engine (like micrograd, 50 lines)

        -- month 2 - Build a mini Autograd Engine 

            --- outcome: implement micrograd-style computational graph 
        
        -- month 3 - Build & Train a 3-layer MLP with Only Tensors 

            --- outcome: verify gradients vs PyTorch autograd 



















